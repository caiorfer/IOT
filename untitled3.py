# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19gn5eGQKelmo_hfmdyW5MoB7NXLH7igE
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
import os
from tensorflow.keras.preprocessing import image

# Definir diretórios dos dados
train_dir = 'path/to/train'
val_dir = 'path/to/val'
test_dir = 'path/to/test'

# Pré-processamento e aumento de dados
train_datagen = ImageDataGenerator(
    rescale=1./255, rotation_range=20, width_shift_range=0.2,
    height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,
    horizontal_flip=True, fill_mode='nearest'
)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Carregar dados de treino, validação e teste
train_generator = train_datagen.flow_from_directory(
    train_dir, target_size=(224, 224),
    batch_size=32, class_mode='categorical'
)
val_generator = val_datagen.flow_from_directory(
    val_dir, target_size=(224, 224),
    batch_size=32, class_mode='categorical'
)
test_generator = test_datagen.flow_from_directory(
    test_dir, target_size=(224, 224),
    batch_size=32, class_mode='categorical'
)

# Carregar o modelo base (VGG16) sem a camada de classificação final
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Congelar as camadas do modelo base
for layer in base_model.layers:
    layer.trainable = False

# Adicionar novas camadas de classificação
x = base_model.output
x = Flatten()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(train_generator.num_classes, activation='softmax')(x)

# Definir o modelo final
model = Model(inputs=base_model.input, outputs=predictions)

# Compilar o modelo
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Callback para salvar o modelo com a melhor precisão
checkpoint_path = "model_checkpoint.h5"
checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1,
                                                save_best_only=True, mode='max')

# Treinar o modelo
history = model.fit(train_generator, steps_per_epoch=train_generator.samples // train_generator.batch_size,
                    validation_data=val_generator, validation_steps=val_generator.samples // val_generator.batch_size,
                    epochs=10, callbacks=[checkpoint])

# Avaliar o modelo com os dados de teste
loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Fazer a predição de múltiplas imagens
def predict_multiple_images(model, image_paths):
    predictions_list = []
    for img_path in image_paths:
        img = image.load_img(img_path, target_size=(224, 224))
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)
        img_array /= 255.0
        predictions = model.predict(img_array)
        predicted_class = np.argmax(predictions[0])
        predictions_list.append(predicted_class)
    return predictions_list

# Exemplo de predição de múltiplas imagens
image_paths = ['path/to/image1.jpg', 'path/to/image2.jpg', 'path/to/image3.jpg']
predicted_classes = predict_multiple_images(model, image_paths)
print('Predicted Classes:', predicted_classes)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# simulando os dados
np.random.seed(0)
data = pd.DataFrame({
    'TemperaturaAgua': np.random.normal(25, 2, 1000),  # temperatura da agua em graus
    'Salinidade': np.random.uniform(25, 35, 1000),     # salinidade em partes por mil
    'pH': np.random.uniform(7, 8.5, 1000),            # ph da água
    'OxigenioDissolvido': np.random.uniform(5, 10, 1000)  # niveis de oxigênio dissolvido em mg/L
})
# ewstatisticas descritivas
print(data.describe())

# correlações entre as variaveis
correlation_matrix = data.corr()
print(correlation_matrix)

# features
X = data[['Salinidade', 'pH', 'OxigenioDissolvido']]

# target
y = data['TemperaturaAgua']

# divisao dos dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# criacao do modelo
model = LinearRegression()

# treinamento do modelo
model.fit(X_train, y_train)

# predicao dos valores
y_pred = model.predict(X_test)

# avaliacao do modelo
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')